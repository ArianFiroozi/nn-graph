digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=20 height=1 label="" ranksep=0.1 shape=box style=filled width=1]
	edge [label=""]
	dpi=300
	conv1 [label="conv1

weight: [8, 1, 1]
in_channels: 1
out_channels: 8
kernel_size: (1,)
stride: (1,)
padding: (0,)
dilation: (1,)
transposed: False
output_padding: (0,)
groups: 1
padding_mode: zeros
_reversed_padding_repeated_twice: (0, 0)
sparsity: 0.0"]
	pool [label="pool

kernel_size: 1
stride: 1
padding: 0
dilation: 1
return_indices: False
ceil_mode: False
"]
	attention [label="attention

in_proj_weight: [24, 8]
q_proj_weight: None
k_proj_weight: None
v_proj_weight: None
in_proj_bias: [24]
embed_dim: 8
kdim: 8
vdim: 8
_qkv_same_embed_dim: True
num_heads: 2
dropout: 0.0
batch_first: False
head_dim: 4
bias_k: None
bias_v: None
add_zero_attn: False
"]
	"attention.out_proj" [label="attention.out_proj

weight: [8, 8]
bias: [8]
in_features: 8
out_features: 8
sparsity: 0.0"]
	fc1 [label="fc1

weight: [32, 8]
bias: [32]
in_features: 8
out_features: 32
sparsity: 0.0"]
	fc2 [label="fc2

weight: [10, 32]
bias: [10]
in_features: 32
out_features: 10
sparsity: 0.0"]
	relu [label="relu

inplace: False
"]
	sigmoid [label="sigmoid

"]
	softmax [label="softmax

dim: 1
"]
	glu [label="glu

dim: 1
"]
	activations [label="activations

fc2: ('relu', ReLU())
"]
	conv1 -> pool
	pool -> attention
	attention -> "attention.out_proj"
	"attention.out_proj" -> fc1
	fc1 -> fc2
	fc2 -> relu
	relu -> sigmoid
	sigmoid -> softmax
	softmax -> glu
	glu -> activations
}
